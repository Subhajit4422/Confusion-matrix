{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Compute performance metrics for the given data '5_a.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a=pd.read_csv(\"5_a.csv\")\n",
    "df_b=pd.read_csv(\"5_b.csv\")\n",
    "df_c=pd.read_csv(\"5_c.csv\")\n",
    "df_d=pd.read_csv(\"5_d.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.637387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.567012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.650230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  1.0  0.637387\n",
       "1  1.0  0.635165\n",
       "2  1.0  0.766586\n",
       "3  1.0  0.724564\n",
       "4  1.0  0.889199\n",
       "5  1.0  0.601600\n",
       "6  1.0  0.666323\n",
       "7  1.0  0.567012\n",
       "8  1.0  0.650230\n",
       "9  1.0  0.829346"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a['y_pred']=df_a['proba'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "df_b['y_pred']=df_b['proba'].apply(lambda x:0 if x<=0.5 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 10000, 0, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_matrix(df):\n",
    "    tn=len(df[(df['y']==0) & (df['y_pred']==0)])\n",
    "    tp=len(df[(df['y']==1) & (df['y_pred']==1)])\n",
    "    fn=len(df[(df['y']==1) & (df['y_pred']==0)])\n",
    "    fp=len(df[(df['y']==0) & (df['y_pred']==1)])\n",
    "    \n",
    "    return tn,tp,fn,fp\n",
    "\n",
    "confusion_matrix(df_a)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(df):\n",
    "    tn,tp,fn,fp=confusion_matrix(df)\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    f1=2*((precision*recall)/(precision+recall))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy value**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    tn,tp,fn,fp=confusion_matrix(df)\n",
    "    acc=((tp+tn)/(tp+fp+fn+tn))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUC Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(df):\n",
    "    tpr=[]\n",
    "    fpr=[]\n",
    "    \n",
    "    df_sort=df.sort_values('proba',ascending=False)\n",
    "    for i in range(0,len(df_sort)):\n",
    "        #https://numpy.org/doc/stable/reference/generated/numpy.where.html\n",
    "        #ith row and [proba] as column\n",
    "        df_sort['y_pred']=np.where(df_sort['proba']>=df_sort.iloc[i]['proba'],1,0) \n",
    "        tn,tp,fn,fp=confusion_matrix(df)\n",
    "        \n",
    "        fp_rate=fp/(tn+fp)\n",
    "        tp_rate=tp/(tp+fn)\n",
    "        tpr.append(tp_rate)\n",
    "        fpr.append(fp_rate)\n",
    "        auc=np.trapz(tpr, fpr) \n",
    "    return auc    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Compute performance metrics for the given data '5_a.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **1.Compute Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Nagative : 0\n",
      "False Positive : 100\n",
      "True Nagative : 0\n",
      "True Positive : 10000\n"
     ]
    }
   ],
   "source": [
    "tn,tp,fn,fp=confusion_matrix(df_a)\n",
    "print(\"False Nagative :\",fn)\n",
    "print(\"False Positive :\",fp)\n",
    "print(\"True Nagative :\",tn)\n",
    "print(\"True Positive :\",tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Compute F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score : 0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "f1=f1_score(df_a)\n",
    "print(\"F1 Score :\",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Compute AUC Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : 0.0\n"
     ]
    }
   ],
   "source": [
    "auc=auc_score(df_a)\n",
    "print(\"AUC Score :\",auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Compute Accuracy Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "acc=accuracy(df_a)\n",
    "print(\"Accuracy Score:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.Compute performance metrics for the given data '5_b.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Compute Confusion Matrix **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Nagative : 45\n",
      "False Positive : 239\n",
      "True Nagative : 9761\n",
      "True Positive : 55\n"
     ]
    }
   ],
   "source": [
    "tn,tp,fn,fp=confusion_matrix(df_b)\n",
    "print(\"False Nagative :\",fn)\n",
    "print(\"False Positive :\",fp)\n",
    "print(\"True Nagative :\",tn)\n",
    "print(\"True Positive :\",tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Compute F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score : 0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "f1=f1_score(df_b)\n",
    "print(\"F1 Score :\",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Compute AUC Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : 0.0\n"
     ]
    }
   ],
   "source": [
    "auc=auc_score(df_b)\n",
    "print(\"AUC Score :\",auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Compute Accuracy Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "acc=accuracy(df_b)\n",
    "print(\"Accuracy Score:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.Compute the best threshold (similarly to ROC curve computation)<br>of probability which gives lowest values of metric A for the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold(df):\n",
    "    uniq_prob=0;\n",
    "    thres_prob=[]\n",
    "    A=[]\n",
    "    \n",
    "    #sorting data based on probability\n",
    "    df_sort=df.sort_values(\"prob\",ascending=False)\n",
    "    for i in range(0,len(df_sort)):\n",
    "        # checking unique probability\n",
    "        if uniq_prob==(df_sort.iloc[i]['prob']):\n",
    "            continue\n",
    "        uniq_prob=df_sort.iloc[i]['prob']\n",
    "        thres_prob.append(uniq_prob)\n",
    "        #Always comparing with last element of thresold-list as it is increasing\n",
    "        df_sort['y_pred']=np.where(df_sort['prob']>=thres_prob[-1],0,1)\n",
    "        # calculating confusion matrix for each threshold\n",
    "        tn,tp,fn,fp=confusion_matrix(df_sort)\n",
    "        val=500*fn+100*fp\n",
    "        A.append(val)\n",
    "        \n",
    "    idx=A.index(min(A))    \n",
    "    return thres_prob[idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best thresold value : 0.9577467989277196\n"
     ]
    }
   ],
   "source": [
    "b=best_threshold(df_c)\n",
    "print('Best thresold value :',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.Compute performance metrics(for regression) for the given data 5_d.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_performance(df):\n",
    "    n=len(df)\n",
    "    #calculate ei=y-y^\n",
    "    df['ei']= df.apply(lambda x: abs(x['y'] - x['pred']), axis=1) #\n",
    "    #square of ei\n",
    "    df['mse']= df['ei'].apply(lambda x: x*x) \n",
    "    total=df['mse'].sum()\n",
    "    #final mse\n",
    "    mse=total/n\n",
    "    #mape=sum(ei)/sum(actual value y)\n",
    "    mape=(df['ei'].sum())/(df['y'].sum())\n",
    "    #simple mean of y\n",
    "    mean=(df['y'].sum())/n\n",
    "    #sum(ei^2)\n",
    "    ssres=df['mse'].sum()\n",
    "    df['sstotal']= df.apply(lambda x: (x['y'] - mean), axis=1)\n",
    "    df['sstotal']= df['sstotal'].apply(lambda x: x*x)\n",
    "    sstotal=df['sstotal'].sum()\n",
    "    r_squared=1-(ssres/sstotal)\n",
    "    df.head(10)\n",
    "    return mse,mape,r_squared   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error : 177.16569974554707\n",
      "Mean absolute percentage error : 12.91202994009687\n",
      "R squared : 0.9563582786990937\n"
     ]
    }
   ],
   "source": [
    "mse,mape,r_squared=regression_performance(df_d)\n",
    "print('Mean squared error :',mse)\n",
    "print('Mean absolute percentage error :',mape*100)\n",
    "print('R squared :',r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
